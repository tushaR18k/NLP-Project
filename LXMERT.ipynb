{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb6e97e0-1220-4526-9e0e-382a054a0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b30c2f4-999f-4b6c-8b7f-486d3b12d2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /afs/cs.pitt.edu/usr0/arr159/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, Image, display\n",
    "import PIL.Image\n",
    "import io\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from _lxmert.processing_image import Preprocess\n",
    "from _lxmert.visualizing_image import SingleImageViz\n",
    "from _lxmert.modeling_frcnn import GeneralizedRCNN\n",
    "from _lxmert._utils import Config\n",
    "import _lxmert._utils\n",
    "from transformers import LxmertForQuestionAnswering, LxmertTokenizer\n",
    "import wget\n",
    "import pickle\n",
    "import os\n",
    "from transformers import LxmertForPreTraining\n",
    "# for visualizing output\n",
    "def showarray(a, fmt=\"jpeg\"):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = io.BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "    \n",
    "lxmert_base = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\").to(device)\n",
    "# load models and model components\n",
    "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "\n",
    "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
    "\n",
    "image_preprocess = Preprocess(frcnn_cfg)\n",
    "\n",
    "lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "badc0d01-37c2-40b0-b84b-e03a7a82de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _lxmert\n",
    "OBJ_URL = \"https://raw.githubusercontent.com/airsplay/py-bottom-up-attention/master/demo/data/genome/1600-400-20/objects_vocab.txt\"\n",
    "objids = _lxmert._utils.get_data(OBJ_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a069ce8-bd7a-45b4-88d7-aaa65c2b5b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "935ba843-82d3-4f8a-8941-d2f5768a5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "def get_object_detection_output(img_paths):\n",
    "    images, sizes, scales_yx = image_preprocess(img_paths)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=frcnn_cfg.max_detections,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return output_dict\n",
    "def get_objects(output_dict):\n",
    "    objects=[]\n",
    "    for tmp in zip(output_dict.get(\"obj_ids\"),output_dict.get(\"obj_probs\")):\n",
    "        objects.append([objids[i] for i, p in zip(tmp[0].tolist(), tmp[1].tolist()) if p > 0.5])\n",
    "    return objects\n",
    "def pretrained_model_fwd_pass(img_paths, txt):\n",
    "    output_dict=get_object_detection_output(img_paths)\n",
    "    objects=get_objects(output_dict)\n",
    "    normalized_boxes = output_dict.get(\"normalized_boxes\").to(device)\n",
    "    features = output_dict.get(\"roi_features\").to(device)\n",
    "    inputs = lxmert_tokenizer(\n",
    "        txt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    cross_relationship_score= nn.Sigmoid()(lxmert_base(input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        visual_feats=features,\n",
    "        visual_pos=normalized_boxes,\n",
    "        token_type_ids=inputs.token_type_ids,\n",
    "        output_attentions=False)['cross_relationship_score'])\n",
    "    shuffled_ids=list(range(len(txt)))\n",
    "    random.shuffle(shuffled_ids)\n",
    "    inputs = lxmert_tokenizer(\n",
    "        [txt[_id] for _id in shuffled_ids],\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    actually_different=[shuffled_ids[_id] == _id for _id in list(range(len(txt)))]\n",
    "    random_cross_relationship_score= nn.Sigmoid()(lxmert_base(input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        visual_feats=features,\n",
    "        visual_pos=normalized_boxes,\n",
    "        token_type_ids=inputs.token_type_ids,\n",
    "        output_attentions=False)['cross_relationship_score'])\n",
    "    return objects, cross_relationship_score.tolist(), random_cross_relationship_score.tolist(), actually_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9eaceeba-6d28-4cc3-885f-05c58578a166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cf711346-53dc-4d6f-a1b5-1a0662507af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib \n",
    "import Dataset\n",
    "importlib.reload(Dataset)\n",
    "from Dataset import ReduceMAMIDataset, collate2\n",
    "num_epochs = 30\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.485, 0.456, 0.406),\n",
    "#                           (0.229, 0.224, 0.225))])\n",
    "from params import *\n",
    "train_dataset = ReduceMAMIDataset(MAX_LEN, MAX_VOCAB, split='train', path_to_dataset='./Data/MASKED_TEXT_TRAINING', transform=None)\n",
    "val_dataset = ReduceMAMIDataset(MAX_LEN, MAX_VOCAB, split='val', path_to_dataset='./Data/MASKED_TEXT_TRAINING', transform=None)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, num_workers=8, collate_fn=collate2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate2)\n",
    "\n",
    "dataloader = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "\n",
    "num_classes = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05724fd-a0a4-4720-b0a2-a584f5143313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f321a5-a9b9-4a64-a248-c0b97fa6a86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6c23184c-b4ee-4b04-a1e1-ac3b6c53ac53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['nose',\n",
       "   'ear',\n",
       "   'face',\n",
       "   'dog',\n",
       "   'eye',\n",
       "   'eye',\n",
       "   'eye',\n",
       "   'eye',\n",
       "   'mouth',\n",
       "   'hair',\n",
       "   'paw',\n",
       "   'paw',\n",
       "   'mouth',\n",
       "   'hand'],\n",
       "  ['eye',\n",
       "   'eye',\n",
       "   'face',\n",
       "   'eye',\n",
       "   'face',\n",
       "   'face',\n",
       "   'mouth',\n",
       "   'nose',\n",
       "   'dog',\n",
       "   'face',\n",
       "   'mouth',\n",
       "   'dog',\n",
       "   'mouth']],\n",
       " [[0.861376166343689, 0.16290029883384705],\n",
       "  [0.5193160772323608, 0.4576598107814789]],\n",
       " [[0.861376166343689, 0.16290029883384705],\n",
       "  [0.5193160772323608, 0.4576598107814789]],\n",
       " [True, True])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_data=[train_dataset[0], train_dataset[1]]\n",
    "batch_img_id, batch_img_paths, batch_text=collate2(toy_data)\n",
    "pretrained_model_fwd_pass(batch_img_paths, batch_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf42747f-e451-455e-bea9-9f1f4ba38b90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shuffle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26231/3565499793.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs = lxmert_tokenizer(\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shuffle' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = lxmert_tokenizer(\n",
    "        shuffle(batch_text),\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "653cb596-59f2-48a6-a30f-70bed4663d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  5086,  1037, 17074,  2138,  1045,  2001,  2894,  2006,\n",
       "          2026,  5798, 22843,  2014,  2005,  2321,  2781,  2059,  2081,  2014,\n",
       "          2079,  1996, 10447,  1998, 11641,  2005,  1996,  2717,  1997,  1996,\n",
       "          3178,  2033,  4168,  3993,  1012,  4012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2023,  7966,  2056,  2026,  8407,  2298,  2066,  2070,  6167,\n",
       "          4842,  2725,  1037,  3975,  1999,  2070, 17074,  6879,  1998,  2085,\n",
       "          1045,  2064,  1005,  1056,  4895, 19763,  2009,  1012,  1030, 14405,\n",
       "          9299,  2015,  2005,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd78f7c8-8a7f-4ea8-845f-6e0b45b947d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "coco_data_path='/own_files/datasets/mscoco/train2014/'\n",
    "with open(f'/afs/cs.pitt.edu/usr0/arr159/erhan_code/t/t/data/COCO2017_train_capdata.pkl', 'rb') as f:\n",
    "    coco_df=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd70a8f2-dc36-448c-869c-9a3716fd31d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a long table with a flower arrangement in the middle for meetings'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "caption, _, _, _= random.choice(coco_df['capdata'][0])\n",
    "caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b64c4490-65c8-4999-ba96-0755bfde6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import os\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, path='/own_files/datasets/mscoco'):\n",
    "        with open(f'/afs/cs.pitt.edu/usr0/arr159/erhan_code/t/t/data/COCO2017_train_capdata.pkl', 'rb') as f:\n",
    "            df=pickle.load(f)\n",
    "            \n",
    "        self.ids=df['path']\n",
    "        self.captions=df['capdata']\n",
    "        self.img_paths=[self.get_path(path, img_pth)  for img_pth in df['path']]\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def get_path(self, path, filename):\n",
    "        return os.path.join(path, ('train2014' if 'train2014' in filename else 'val2014'), filename)\n",
    "    def __getitem__(self, i):\n",
    "        caption, _, _, _= random.choice(self.captions[i])# extract cap\n",
    "        return (self.ids[i], caption, self.img_paths[i])\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        ids=[data[0] for data in batch]\n",
    "        img_paths=[data[2] for data in batch]\n",
    "        captions=[data[1] for data in batch]\n",
    "        return ids, img_paths, captions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d48fed22-ab1e-43f1-ad6f-3dd6814fbf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('COCO_train2014_000000454708.jpg',\n",
       " 'a computer sits on a l shaped desk',\n",
       " '/own_files/datasets/mscoco/train2014/COCO_train2014_000000454708.jpg')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COCODataset()[7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6886c3-41d7-4dab-b79e-67da384055f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
