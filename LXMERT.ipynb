{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb6e97e0-1220-4526-9e0e-382a054a0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b30c2f4-999f-4b6c-8b7f-486d3b12d2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /afs/cs.pitt.edu/usr0/arr159/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, Image, display\n",
    "import PIL.Image\n",
    "import io\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from _lxmert.processing_image import Preprocess\n",
    "from _lxmert.visualizing_image import SingleImageViz\n",
    "from _lxmert.modeling_frcnn import GeneralizedRCNN\n",
    "from _lxmert._utils import Config\n",
    "import _lxmert._utils\n",
    "from transformers import LxmertForQuestionAnswering, LxmertTokenizer\n",
    "import wget\n",
    "import pickle\n",
    "import os\n",
    "from transformers import LxmertForPreTraining\n",
    "# for visualizing output\n",
    "def showarray(a, fmt=\"jpeg\"):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = io.BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "    \n",
    "lxmert_base = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\").to(device)\n",
    "# load models and model components\n",
    "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "\n",
    "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
    "\n",
    "image_preprocess = Preprocess(frcnn_cfg)\n",
    "\n",
    "lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "badc0d01-37c2-40b0-b84b-e03a7a82de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _lxmert\n",
    "OBJ_URL = \"https://raw.githubusercontent.com/airsplay/py-bottom-up-attention/master/demo/data/genome/1600-400-20/objects_vocab.txt\"\n",
    "objids = _lxmert._utils.get_data(OBJ_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a069ce8-bd7a-45b4-88d7-aaa65c2b5b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "935ba843-82d3-4f8a-8941-d2f5768a5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "def get_object_detection_output(img_paths):\n",
    "    images, sizes, scales_yx = image_preprocess(img_paths)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=frcnn_cfg.max_detections,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return output_dict\n",
    "def get_objects(output_dict):\n",
    "    objects=[]\n",
    "    for tmp in zip(output_dict.get(\"obj_ids\"),output_dict.get(\"obj_probs\")):\n",
    "        objects.append([objids[i] for i, p in zip(tmp[0].tolist(), tmp[1].tolist()) if p > 0.5])\n",
    "    return objects\n",
    "def pretrained_model_fwd_pass(img_paths, txt):\n",
    "    output_dict=get_object_detection_output(img_paths)\n",
    "    objects=get_objects(output_dict)\n",
    "    normalized_boxes = output_dict.get(\"normalized_boxes\").to(device)\n",
    "    features = output_dict.get(\"roi_features\").to(device)\n",
    "    inputs = lxmert_tokenizer(\n",
    "        txt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    cross_relationship_score= nn.Sigmoid()(lxmert_base(input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        visual_feats=features,\n",
    "        visual_pos=normalized_boxes,\n",
    "        token_type_ids=inputs.token_type_ids,\n",
    "        output_attentions=False)['cross_relationship_score'])\n",
    "    shuffled_ids=list(range(len(txt)))\n",
    "    random.shuffle(shuffled_ids)\n",
    "    inputs = lxmert_tokenizer(\n",
    "        [txt[_id] for _id in shuffled_ids],\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    actually_different=[shuffled_ids[_id] == _id for _id in list(range(len(txt)))]\n",
    "    random_cross_relationship_score= nn.Sigmoid()(lxmert_base(input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        visual_feats=features,\n",
    "        visual_pos=normalized_boxes,\n",
    "        token_type_ids=inputs.token_type_ids,\n",
    "        output_attentions=False)['cross_relationship_score'])\n",
    "    return objects, cross_relationship_score.tolist(), random_cross_relationship_score.tolist(), actually_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9eaceeba-6d28-4cc3-885f-05c58578a166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf711346-53dc-4d6f-a1b5-1a0662507af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /afs/cs.pitt.edu/usr0/arr159/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /afs/cs.pitt.edu/usr0/arr159/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import importlib \n",
    "import Dataset\n",
    "importlib.reload(Dataset)\n",
    "from Dataset import ReduceMAMIDataset, collate2\n",
    "num_epochs = 30\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.485, 0.456, 0.406),\n",
    "#                           (0.229, 0.224, 0.225))])\n",
    "from params import *\n",
    "train_dataset = ReduceMAMIDataset(MAX_LEN, MAX_VOCAB, split='train', path_to_dataset='./Data/MASKED_TEXT_TRAINING', transform=None)\n",
    "val_dataset = ReduceMAMIDataset(MAX_LEN, MAX_VOCAB, split='val', path_to_dataset='./Data/MASKED_TEXT_TRAINING', transform=None)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, num_workers=8, collate_fn=collate2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate2)\n",
    "\n",
    "dataloader = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "\n",
    "num_classes = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05724fd-a0a4-4720-b0a2-a584f5143313",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ReduceIronicMEMEDataset(MAX_LEN, MAX_VOCAB, path_to_dataset_folder='./Data/Misogynistic-MEME_pt1', path_to_data={'images':'Images', 'csv':'table.csv'}, transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f321a5-a9b9-4a64-a248-c0b97fa6a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "path_to_dataset_folder='./Data/Misogynistic-MEME_pt1'\n",
    "path_to_data={'images':'Images', 'csv':'table.csv'}\n",
    "df = data = pd.read_csv(os.path.join(path_to_dataset_folder, path_to_data['csv']),sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6a44397-944f-4411-a08d-9a1f15f5bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23714/398518272.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df[df['misogynisticDE']==1][df['ironicDE']==1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>memeID</th>\n",
       "      <th>text</th>\n",
       "      <th>misogynisticDE</th>\n",
       "      <th>aggressiveDE</th>\n",
       "      <th>ironicDE</th>\n",
       "      <th>misogynisticCS</th>\n",
       "      <th>aggressiveCS</th>\n",
       "      <th>ironicCS</th>\n",
       "      <th>confidence_M_CS</th>\n",
       "      <th>confidence_A_CS</th>\n",
       "      <th>confidence_I_CS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENG01</td>\n",
       "      <td>The way every man feels when a woman is driving</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6716</td>\n",
       "      <td>0.6716</td>\n",
       "      <td>0.6716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG02</td>\n",
       "      <td>Women yes it's a woman</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENG03</td>\n",
       "      <td>I dont' understand, this car has 3 pedals and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENG04</td>\n",
       "      <td>They said I didn't belong in the garage so I b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6672</td>\n",
       "      <td>0.6672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENG05</td>\n",
       "      <td>Women drivers it's the only possible explanation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6664</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>ENG390</td>\n",
       "      <td>The floor is women rights</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6623</td>\n",
       "      <td>0.6841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>ENG391</td>\n",
       "      <td>Lol basically. Women, men.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6820</td>\n",
       "      <td>0.6820</td>\n",
       "      <td>0.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>ENG392</td>\n",
       "      <td>Stupid slow driver.. o its a woman</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>ENG393</td>\n",
       "      <td>what men play with vs what women play with</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6768</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>ENG396</td>\n",
       "      <td>Women - good guy. douchebag women</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     memeID                                               text  \\\n",
       "0     ENG01    The way every man feels when a woman is driving   \n",
       "1     ENG02                             Women yes it's a woman   \n",
       "2     ENG03  I dont' understand, this car has 3 pedals and ...   \n",
       "3     ENG04  They said I didn't belong in the garage so I b...   \n",
       "4     ENG05   Women drivers it's the only possible explanation   \n",
       "..      ...                                                ...   \n",
       "389  ENG390                          The floor is women rights   \n",
       "390  ENG391                         Lol basically. Women, men.   \n",
       "391  ENG392                 Stupid slow driver.. o its a woman   \n",
       "392  ENG393         what men play with vs what women play with   \n",
       "395  ENG396                  Women - good guy. douchebag women   \n",
       "\n",
       "     misogynisticDE  aggressiveDE  ironicDE  misogynisticCS  aggressiveCS  \\\n",
       "0                 1             0         1               1             0   \n",
       "1                 1             0         1               1             0   \n",
       "2                 1             0         1               1             0   \n",
       "3                 1             1         1               1             0   \n",
       "4                 1             0         1               1             0   \n",
       "..              ...           ...       ...             ...           ...   \n",
       "389               1             1         1               1             0   \n",
       "390               1             0         1               1             0   \n",
       "391               1             1         1               1             0   \n",
       "392               1             0         1               0             0   \n",
       "395               1             1         1               1             0   \n",
       "\n",
       "     ironicCS  confidence_M_CS  confidence_A_CS confidence_I_CS  \n",
       "0           1           0.6716           0.6716          0.6716  \n",
       "1           1           1.0000           1.0000             1.0  \n",
       "2           1           1.0000           1.0000             1.0  \n",
       "3           1           1.0000           0.6672          0.6672  \n",
       "4           1           1.0000           0.6664             1.0  \n",
       "..        ...              ...              ...             ...  \n",
       "389         1           1.0000           0.6623          0.6841  \n",
       "390         1           0.6820           0.6820           0.682  \n",
       "391         1           1.0000           1.0000          0.6587  \n",
       "392         0           0.6768           0.0000             0.0  \n",
       "395         1           1.0000           0.6582          0.6582  \n",
       "\n",
       "[145 rows x 11 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['misogynisticDE']==1][df['ironicDE']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6c23184c-b4ee-4b04-a1e1-ac3b6c53ac53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['nose',\n",
       "   'ear',\n",
       "   'face',\n",
       "   'dog',\n",
       "   'eye',\n",
       "   'eye',\n",
       "   'eye',\n",
       "   'eye',\n",
       "   'mouth',\n",
       "   'hair',\n",
       "   'paw',\n",
       "   'paw',\n",
       "   'mouth',\n",
       "   'hand'],\n",
       "  ['eye',\n",
       "   'eye',\n",
       "   'face',\n",
       "   'eye',\n",
       "   'face',\n",
       "   'face',\n",
       "   'mouth',\n",
       "   'nose',\n",
       "   'dog',\n",
       "   'face',\n",
       "   'mouth',\n",
       "   'dog',\n",
       "   'mouth']],\n",
       " [[0.861376166343689, 0.16290029883384705],\n",
       "  [0.5193160772323608, 0.4576598107814789]],\n",
       " [[0.861376166343689, 0.16290029883384705],\n",
       "  [0.5193160772323608, 0.4576598107814789]],\n",
       " [True, True])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_data=[train_dataset[0], train_dataset[1]]\n",
    "batch_img_id, batch_img_paths, batch_text=collate2(toy_data)\n",
    "pretrained_model_fwd_pass(batch_img_paths, batch_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf42747f-e451-455e-bea9-9f1f4ba38b90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shuffle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26231/3565499793.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs = lxmert_tokenizer(\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shuffle' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = lxmert_tokenizer(\n",
    "        shuffle(batch_text),\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "653cb596-59f2-48a6-a30f-70bed4663d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  5086,  1037, 17074,  2138,  1045,  2001,  2894,  2006,\n",
       "          2026,  5798, 22843,  2014,  2005,  2321,  2781,  2059,  2081,  2014,\n",
       "          2079,  1996, 10447,  1998, 11641,  2005,  1996,  2717,  1997,  1996,\n",
       "          3178,  2033,  4168,  3993,  1012,  4012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2023,  7966,  2056,  2026,  8407,  2298,  2066,  2070,  6167,\n",
       "          4842,  2725,  1037,  3975,  1999,  2070, 17074,  6879,  1998,  2085,\n",
       "          1045,  2064,  1005,  1056,  4895, 19763,  2009,  1012,  1030, 14405,\n",
       "          9299,  2015,  2005,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd78f7c8-8a7f-4ea8-845f-6e0b45b947d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "coco_data_path='/own_files/datasets/mscoco/train2014/'\n",
    "with open(f'/afs/cs.pitt.edu/usr0/arr159/erhan_code/t/t/data/COCO2017_train_capdata.pkl', 'rb') as f:\n",
    "    coco_df=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd70a8f2-dc36-448c-869c-9a3716fd31d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a long table with a flower arrangement in the middle for meetings'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "caption, _, _, _= random.choice(coco_df['capdata'][0])\n",
    "caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b64c4490-65c8-4999-ba96-0755bfde6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, path='/own_files/datasets/mscoco'):\n",
    "        with open(f'/afs/cs.pitt.edu/usr0/arr159/erhan_code/t/t/data/COCO2017_train_capdata.pkl', 'rb') as f:\n",
    "            df=pickle.load(f)\n",
    "            \n",
    "        self.ids=df['path']\n",
    "        self.captions=df['capdata']\n",
    "        self.img_paths=[self.get_path(path, img_pth)  for img_pth in df['path']]\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def get_path(self, path, filename):\n",
    "        return os.path.join(path, ('train2014' if 'train2014' in filename else 'val2014'), filename)\n",
    "    def __getitem__(self, i):\n",
    "        caption, _, _, _= random.choice(self.captions[i])# extract cap\n",
    "        return (self.ids[i], caption, self.img_paths[i])\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        ids=[data[0] for data in batch]\n",
    "        img_paths=[data[2] for data in batch]\n",
    "        captions=[data[1] for data in batch]\n",
    "        return ids, img_paths, captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d48fed22-ab1e-43f1-ad6f-3dd6814fbf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('coco_captions.pickle', 'wb') as f:\n",
    "    captions=COCODataset().captions\n",
    "    captions=[random.choice(i)[0] for i in captions]# extract cap\n",
    "    \n",
    "    pickle.dump(captions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d6886c3-41d7-4dab-b79e-67da384055f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lxmert_processed_files=[\"masked_df_ironic_meme_lxmert.csv\",\"coco_lxmert.csv\", \"non_masked_df_ironic_meme_lxmert.csv\", \"masked_df_mami_lxmert2.csv\", \"not_masked_df_mami_lxmert2.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "017077c0-a369-4f77-9ff6-4705bcc857d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to this github issue, using the second logit as a match - https://github.com/huggingface/transformers/issues/7266#issuecomment-748211135 \n",
    "import re\n",
    "def fix_dataframe(path_to_df):\n",
    "    def process_str_arr(x):\n",
    "        return [float(i) for i in re.split('\\[|\\]|,',x) if i != '']\n",
    "\n",
    "    df=pd.read_csv(path_to_df)\n",
    "    \n",
    "    df['sim_match']=df['sim'].apply(lambda x: process_str_arr(x)[1])\n",
    "    df['sim_mismatch']=df['sim'].apply(lambda x: process_str_arr(x)[0])\n",
    "    df['random_sim_match']=df['random_sims'].apply(lambda x: process_str_arr(x)[1])\n",
    "    df['random_sim_mismatch']=df['random_sims'].apply(lambda x: process_str_arr(x)[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68ef2777-9595-4fea-99cf-aadd264b5b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_df_ironic_meme_lxmert.csv\n",
      "0.2521 0.1392\n",
      "0.2243 0.124\n",
      "coco_lxmert.csv\n",
      "0.4639 0.3108\n",
      "0.0242 0.0625\n",
      "non_masked_df_ironic_meme_lxmert.csv\n",
      "0.2881 0.1512\n",
      "0.2594 0.1339\n",
      "masked_df_mami_lxmert2.csv\n",
      "0.2673 0.1274\n",
      "0.247 0.1139\n",
      "not_masked_df_mami_lxmert2.csv\n",
      "0.323 0.1463\n",
      "0.3106 0.1463\n"
     ]
    }
   ],
   "source": [
    "for file_name in lxmert_processed_files:\n",
    "    print(file_name)\n",
    "    df=fix_dataframe(file_name)\n",
    "    print(round(df['sim_match'].mean(), 4), round(df['sim_match'].std(), 4))\n",
    "    print(round(df['random_sim_match'].mean(), 4), round(df['random_sim_match'].std(), 4))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f79ac358-3d36-41fd-b36e-350fe41ba442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=fix_dataframe('masked_df_ironic_meme_lxmert.csv')\n",
    "meta_data_df=pd.read_csv('Data/MASKED_IRONIC_MEME_TEXT_TRAINING/table.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79bd3bca-3a79-4400-958c-c0b21900c705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['memeID', 'text', 'misogynisticDE', 'aggressiveDE', 'ironicDE',\n",
       "       'misogynisticCS', 'aggressiveCS', 'ironicCS', 'confidence_M_CS',\n",
       "       'confidence_A_CS', 'confidence_I_CS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2f290f40-3055-46df-9ea9-38ad3db00942",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_df#['ironicDE']\n",
    "df['memeID']=df['img_ids'].apply(lambda x: re.split('res_|.jpg',x)[-2])\n",
    "\n",
    "merged_df=meta_data_df.merge(df, on='memeID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c8f7c269-3b18-4983-8c36-f598d71382d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_by_attr(attribute):\n",
    "    condition=merged_df[attribute]==1\n",
    "    print('With', attribute, ': sim:', round(merged_df[condition]['sim_match'].mean(),4), round(merged_df[condition]['sim_match'].std(),4), 'random sim:', round(merged_df[condition]['random_sim_match'].mean(),4), round(merged_df[condition]['random_sim_match'].std(), 4))\n",
    "    condition=merged_df[attribute]==0\n",
    "    print('Without', attribute, ': sim:', round(merged_df[condition]['sim_match'].mean(), 4), round(merged_df[condition]['sim_match'].std(), 4), 'random sim:', round(merged_df[condition]['random_sim_match'].mean(), 4), round(merged_df[condition]['random_sim_match'].std(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d63219-e28e-4fde-ae86-a77281080e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "91699904-4e8b-4f54-bed3-8d6163f6dec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean and std\n",
      "With misogynisticDE : sim: 0.2543 0.155 random sim: 0.2376 0.1328\n",
      "Without misogynisticDE : sim: 0.2499 0.1216 random sim: 0.211 0.1132\n",
      "With ironicDE : sim: 0.2584 0.1594 random sim: 0.2334 0.1274\n",
      "Without ironicDE : sim: 0.2507 0.1345 random sim: 0.2223 0.1233\n"
     ]
    }
   ],
   "source": [
    "print(\"mean and std\")\n",
    "get_scores_by_attr('misogynisticDE')\n",
    "get_scores_by_attr('ironicDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0314d010-cf3d-4514-9200-a7b169e54511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENGN339'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "test_str='[0.993421733379364, 0.006969502195715904]'\n",
    "[float(i) for i in re.split('\\[|\\]|,',test_str) if i != '']\n",
    "#[str_num.split('[]')[0] for str_num in .split()]\n",
    "test_str='res_ENGN339.jpg'\n",
    "re.split('res_|.jpg',test_str)[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "374f64a7-7ab6-4124-b17f-0de009939433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'img_ids', 'sim', 'random_sims', 'img_random_sims',\n",
      "       'objects', 'sim_match', 'sim_mismatch', 'random_sim_match',\n",
      "       'random_sim_mismatch'],\n",
      "      dtype='object')\n",
      "0.323 0.1463\n",
      "0.3106 0.1463\n"
     ]
    }
   ],
   "source": [
    "df=fix_dataframe(file_name)\n",
    "print(df['img_ids'])\n",
    "print(round(df['sim_match'].mean(), 4), round(df['sim_match'].std(), 4))\n",
    "print(round(df['random_sim_match'].mean(), 4), round(df['random_sim_match'].std(), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced73780-af6d-41eb-882b-e77ad745d05b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
